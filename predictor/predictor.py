import csv
import argparse
import os
import numpy as np
import gensim.models.word2vec as wv
import json
import subprocess
import re
import math

from copy import deepcopy
from scipy import spatial
from predictor_funcs import avg_feature_vector, get_line_vuln
from concurrent.futures import ThreadPoolExecutor, wait, ProcessPoolExecutor

def get_dev_block_values(start, end):
    length = (end-start)+1
    results = [0] * length
    if length % 2:
        center = math.floor(length/2)
        results[center] = length
        for i in range(1, center+1):
            results[center+i] = results[center-i] = length - i
    else:
        center = int(length/2)
        for i in range(0, center):
            results[center+i] = results[center-(i+1)] = length - i
    return results

# create a vector that's as long as the amount of features we have(may be an issue)
# if a word is in our model add it to the sentece's feature vector
# get the average
# def avg_feature_vector(sentence, model, num_features, index2word_set):
#     words = sentence.split()
#     feature_vec = np.zeros((num_features, ), dtype='float32')
#     n_words = 0
#     for word in words:
#         if word in index2word_set:
#             n_words += 1
#             feature_vec = np.add(feature_vec, model[word])
#     if (n_words > 0):
#         feature_vec = np.divide(feature_vec, n_words)
#     return feature_vec

# # get the lines minimum distance from one of the already stored vulnerable lines
# # return a tuple of the vuln value and the line that's closest
# def get_line_vuln(args, features=100):
#     (line, vuln_model, vuln_index2word_set, line_vectors, known_vuln_lines) = args
#     mini = 1
#     reason = ''
#     line_vec = avg_feature_vector(line, vuln_model, features, vuln_index2word_set)
#     for vec, line in zip(line_vectors, known_vuln_lines):
#         curr = spatial.distance.cosine(line_vec, vec)
#         if curr < mini:
#             mini = curr
#             reason = line
#     return (mini,reason)

# predict wether a line is vulnerable or not
def predict(target, model, features, index2word_set, acceptance, file_name=None, modelName=None, rules=list(), count = -1):
    line_results = list()

    # if the file doesn't exist fail here
    with open(target, 'r'):
        pass
    # the lexer doesn't empty the corpus, so it is removed here
    if os.path.isfile('./corpus.txt'):
        os.remove('./corpus.txt')

    # lex the file with the current model's corresponding lexer
    subprocess.run(['java', '-jar', '../java_line_pred.jar', target])
    # calc all vuln line vectors
    # create a list of dict's where each dict contains the line, the chance of it being vulnerable, the reason for that given chance, and the lines number

    

    with open(target, 'r', errors='ignore') as fp:
        #work is needed here
        with open('./corpus.txt','r') as fp2:
            multi_input = list()
            for line in fp2.readlines():
                multi_input.append((line, model, index2word_set, vuln_line_vectors, vuln_lines))

            with ProcessPoolExecutor() as executor:
                results = executor.map(get_line_vuln, multi_input)

            for i,(orig_line, result_tuple) in enumerate(zip(fp.readlines(), results)):
                line_results.append(
                    {
                        'line_num' : i,
                        'vuln_prob' : 1 - result_tuple[0],
                        'line' : orig_line.strip(),
                        'reason' : result_tuple[1],
                        'distance': 1 - result_tuple[0],
                        'surrounding': 0,
                        'complexity': 0,
                        'git_surr' : 0,
                        'model': modelName
                    }
                )
    if file_name:
        print(file_name)
        if 'filtered.java' in target:
            filterless_target = '/'.join(target.split('/')[2:-1])
            os.chdir(f'./{modelName}_results'+('' if count == -1 else '_'+str(count))+'/'+filterless_target)
        else:
            os.chdir(f'./{modelName}_results'+('' if count == -1 else '_'+str(count))+f'/{file_name}/'+'_'.join(target.split('/')[-3:]))
    # perform the check for every specified acceptance value
    with ThreadPoolExecutor(max_workers=os.cpu_count() + 4) as executor:
        accept_results = dict()
        for accept in acceptance:
            accept_results[accept] = executor.submit(execute_rules, accept, line_results, rules, target)

        for accept in acceptance:
            # any rules that change the cwd are executed here
            # they are only executed if the base rule list contains them
            if 'rule_check_change_blocks' in accept_results[accept].result().keys():
                accept_results[accept].result()['check_change_blocks'] = [line for line in check_change_blocks(line_results, accept, target) if line['vuln_prob'] > accept]
                
        for accept in accept_results:
            if file_name:
                if not os.path.isdir(f'{int(accept*100)}%'):
                        os.mkdir(f'{int(accept*100)}%')

                os.chdir(f'{int(accept*100)}%')
            vuln_lines_processing_futures = list()
            for rule_name in accept_results[accept].result():
                vuln_lines_processing_futures.append(executor.submit(process_vuln_lines, accept_results[accept].result()[rule_name], rule_name))
            #this is needed so the vuln_line processing tasks finish while the cwd is in the desired location
            wait(vuln_lines_processing_futures)
            os.chdir('../')
        os.chdir(BASEDIR)

def execute_rules(accept, line_results, rules, target):
    rule_results = dict()

    # create a new list of vulnerable lines
    vuln_lines = [line for line in line_results if line['vuln_prob'] > accept]
    # before any rules are applied create output for the base results
    rule_results['no_rule'] = deepcopy(vuln_lines)
    # apply each specified rule
    for rule in rules:
        # this needs to be extended for every rule that changes cwd during it's runtime
        if rule.__name__ != 'check_change_blocks':
            vuln_lines = [line for line in rule(line_results, accept, target) if line['vuln_prob'] > accept]
        rule_results[f'rule_{rule.__name__}'] = deepcopy(vuln_lines)

    return rule_results

# create the output cve file
def process_vuln_lines(vuln_lines, file_name):

    with open(('sus.csv' if not file_name else f'{file_name}.csv'),'w', newline='') as fp:
        writer = csv.DictWriter(fp, fieldnames = ['num','prob','line','reason', 'distance', 'surrounding', 'complexity', 'git_surr'])
        writer.writerow({
                        'num' : 'Line_number',
                        'prob' : 'Chance_of_vuln',
                        'line' : 'Line_content',
                        'reason' : 'Reason_for_pred',
                        'distance': 'distance',
                        'surrounding': 'surrounding',
                        'complexity': 'complexity',
                        'git_surr' : 'git_surr'
                    })
        for line in vuln_lines:
            writer.writerow({
                        'num' : line['line_num'],
                        'prob' : line['vuln_prob'],
                        'line' : line['line'],
                        'reason' : line['reason'],
                        'distance': line['distance'],
                        'surrounding': line['surrounding'],
                        'complexity': line['complexity'],
                        'git_surr' : line['git_surr']
                    })
    print('CSV done')

# reads in all the given vulnerable lines
def readVulnLines(model, features, index2word_set, file_name='vuln_lines_new_lex.txt'):
    with open(file_name, 'r') as fp:
        for line in json.load(fp):
            vuln_lines.append(line)
            vuln_line_vectors.append(avg_feature_vector(line, model, features, index2word_set))

# this function servers to psuh values back to between 0-1
def weight_func(x, n=0):
    return 1 - ((1 + n)/(x + n))

# filters out all lines that only contain 1 word, this is applied permanently, and so it needs to be the first rule
def no_one_word_lines(line_data, acceptance, target):
    for line in line_data:
        if len(line['line'].split()) < 2:
            line['vuln_prob'] = 0
    return line_data
# checks how many vulnerable lines there are in the same vuln_block as the line given
def check_surroundings(line_data, acceptance, target):
    result = deepcopy(line_data)

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break

            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    return result

# prefer more complex lines
def prefer_complex(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()
    for line in result:

        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    return result

# combine prefer_complex and check_surroundings, here the complexity preference is applied after the vuln_blocks are calculated
def complex_and_surrounded(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            if(len(set(lexed_lines[line['line_num']].split()))):
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split()))) + weight_func(neighbourCount)) / 3
                line['complexity'] = len(set(lexed_lines[line['line_num']].split()))
                line['surrounding'] = neighbourCount

    return result

# apply the check surroundings rule multiple times, to see if it converges
def iter_surrounding(line_data, acceptance, target):
    result = deepcopy(line_data)

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break

            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = (line['surrounding'] + neighbourCount) / 2

    for _ in range(0,100):
        prev_state = deepcopy(result)
        for (i, line), data in zip(enumerate(result), prev_state):
            neighbourCount = 1
            if data['vuln_prob'] > acceptance:
                for j in range(i-1,0,-1):
                    if line_data[j]['vuln_prob'] > acceptance:
                        neighbourCount += 1
                    else:
                        break
                for j in range(i+1, len(line_data)-1):
                    if line_data[j]['vuln_prob'] > acceptance:
                        neighbourCount += 1
                    else:
                        break

                line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
                line['surrounding'] = neighbourCount

    return result

# combine prefer_complex and check_surroundings, apply the complex preference before vuln_blocks are calculated
def complex_before_surr(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for line in result:
        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    return result

def complex_before_surr_reg(line_data, acceptance, target):
    result = deepcopy(line_data)

    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    with open(BASEDIR + '/coeffs.json', 'r') as fp:
        coeffs = json.load(fp)

    model_name = ''
    for line in result:
        if (len(set(lexed_lines[line['line_num']].split()))):
            model_name = line['model']
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    selected_values = ''
    for key in coeffs.keys():
        if str(key).startswith(model_name) and str(key).endswith('before_surr.csv'):
            selected_values = coeffs[key]
            break

    parts = selected_values.split('+')
    constant = float(parts[0].strip())
    coeff_distance = float(parts[1].split('*')[0].strip())
    coeff_surrounding = float(parts[2].split('*')[0].strip())
    coeff_complexity = float(parts[3].split('*')[0].strip())

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > 0:
            for j in range(i - 1, 0, -1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            for j in range(i + 1, len(line_data) - 1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    for line in result:
        if constant + coeff_distance * line['distance'] + coeff_surrounding * line['surrounding'] + coeff_complexity * line['complexity'] > acceptance:
            line['vuln_prob'] = 1
        else:
            line['vuln_prob'] = 0

    return result

def complex_and_surrounded_reg(line_data, acceptance, target):
    result = deepcopy(line_data)

    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    with open(BASEDIR + '/coeffs.json', 'r') as fp:
        coeffs = json.load(fp)

    model_name = ''
    for line in result:
        model_name = line['model']
        break

    selected_values = ''
    for key in coeffs.keys():
        if str(key).startswith(model_name) and str(key).endswith('surrounded.csv'):
            selected_values = coeffs[key]
            break

    parts = selected_values.split('+')
    constant = float(parts[0].strip())
    coeff_distance = float(parts[1].split('*')[0].strip())
    coeff_surrounding = float(parts[2].split('*')[0].strip())
    coeff_complexity = float(parts[3].split('*')[0].strip())

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > 0:
            for j in range(i - 1, 0, -1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            for j in range(i + 1, len(line_data) - 1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            if (len(set(lexed_lines[line['line_num']].split()))):
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(
                    len(set(lexed_lines[line['line_num']].split()))) + weight_func(neighbourCount)) / 3
                line['complexity'] = len(set(lexed_lines[line['line_num']].split()))
                line['surrounding'] = neighbourCount

    for line in result:
        if constant + coeff_distance * line['distance'] + coeff_surrounding * line['surrounding'] + coeff_complexity * line['complexity'] > acceptance:
            line['vuln_prob'] = 1
        else:
            line['vuln_prob'] = 0

    return result

def check_change_blocks(line_data, acceptance, target):
    result = deepcopy(line_data)

    current_dir = os.getcwd()

    os.chdir(BASEDIR)

    path = '/'.join(target.split('/')[:-1])

    filename = target.split('/')[-1]

    os.chdir(path)

    lines  = subprocess.getoutput(f'git blame {filename}').split('\n')
    names = list()

    for line in lines:
        pattern = re.compile(r'[0-9]{4}-[0-9]{2}-[0-9]{2}')
        line_elements = line.split()
        endName = [i for i, element in enumerate(line_elements) if pattern.match(element)]
        if len(endName) == 1:
            names.append(' '.join(line_elements[1:endName[0]]))
        else:
            print("ERROR")
            print(endName)

    blockValues = list()

    start = -1
    end = 0
    prev = ''

    for i,name in enumerate(names):
        if start == -1:
            start = 0
            prev = name
        elif i == len(names)-1:
            blockValues += get_dev_block_values(start, end)
        else:
            if prev == name:
                end = i
            else:
                if start == end:
                    blockValues.append(1)
                else:
                    blockValues += get_dev_block_values(start, end)
                start = i
                end = i
                prev = name

    os.chdir(current_dir)

    for res, value in zip(result, blockValues):
        res['vuln_prob'] = (res['vuln_prob'] + (1 - weight_func(value, 10))) / 2
        res['git_surr'] = value

    return result

def new_surrounded(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/corpus.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for line in result:
        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            if neighbourCount > 0:
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            else:
                line['vuln_prob'] = 0
            line['surrounding'] = neighbourCount

    return result

def main():
    # get target path from command line argument
    arg_parser = argparse.ArgumentParser(description='Look for potentially vulnerable lines in a js file')
    arg_parser.add_argument('-f', '--target_file', metavar='', help='Path to the file that needs to be checked')
    args = arg_parser.parse_args()

    # these are treated as global variables in case they would be needed for a rule
    global acceptance, model, index2word_set, vuln_line_vectors, vuln_lines, BASEDIR

    vuln_line_vectors = list()
    acceptance = list()
    model = None
    index2word_set = set()
    vuln_lines = list()
    BASEDIR = os.getcwd()


    # the predictor should be runnable for a specific file, as for now, this uses a preset of the IDname model, 80% acceptance, and no rules
    if args.target_file == 'create_test_results':

        not_found = list()
        # for the creation of test results every available option is explored herefor acceptance values, rules, and models
        acceptance = [0.75, 0.8, 0.85, 0.9, 0.95]

        # the modelNames need to be in the same order as the txt files
        modelNames = ['word2vec']

        vuln_file_names = ['./filtered_vlr.json']

        features = 100

        rules = [no_one_word_lines, check_surroundings, prefer_complex, complex_and_surrounded, complex_before_surr, iter_surrounding, new_surrounded]

        if not os.path.isdir('pred_repos'):
            os.mkdir('pred_repos')

        with open('to_predict.json','r') as fp:
            target_info = json.load(fp)

        for mName, file_name in zip(modelNames, vuln_file_names):
            model = wv.Word2Vec.load(f"models/{mName}.model")

            index2word_set = set(model.wv.index2word)

            if not os.path.isdir(f'./{mName}_results'):
                os.mkdir(f'./{mName}_results')
            readVulnLines(model, features, index2word_set, file_name)
            for proj_name in target_info:

                if not os.path.isdir(f'./pred_repos/{proj_name}'):
                    os.chdir('pred_repos')
                    subprocess.run(['git','clone',target_info[proj_name][0]['commits'][0]['repository']])
                    os.chdir(BASEDIR)
                
                for commit in target_info[proj_name]:
                    for commit_info in commit['commits']:
                        parent_hash = commit_info['parent']
                        target_name = proj_name + '_' + commit_info['id']

                        if not os.path.isdir(f'./filtered_files/{target_name}'):
                            os.chdir(f'./pred_repos/{proj_name}')
                            subprocess.run(['git','checkout', parent_hash,'-f'])
                            os.chdir(BASEDIR)
                        if not os.path.isdir(f'./{mName}_results/{target_name}'):
                            os.mkdir(f'./{mName}_results/{target_name}')

                        for path_info in commit['vuln_lines'][commit_info['id']]:
                            target = './pred_repos/' + proj_name + path_info.split(' ')[1][1:]
                            folder_name = '_'.join(target.split('/')[-3:])
                            if os.path.isdir(f'./filtered_files/{target_name}/{folder_name}'):
                                target = f'./filtered_files/{target_name}/{folder_name}/filtered.java'
                            if not os.path.isdir(f'./{mName}_results/{target_name}/'+folder_name):
                                os.mkdir(f'./{mName}_results/{target_name}/'+folder_name)
                            print(f'Begin {mName} test on {target_name}\'s file on path:{target}')
                            predict(target, model, features, index2word_set, acceptance, target_name, mName, rules)
                            not_found.append(target)
        with open('not_found.json', 'w') as fp:
            json.dump(not_found, fp, indent=4)
    elif args.target_file == 'filter_files':
        with open('to_predict.json','r') as fp:
            target_info = json.load(fp)

        if not os.path.isdir('./filtered_files'):
            os.mkdir('filtered_files')
        for proj_name in target_info:
            if not os.path.isdir(f'./pred_repos/{proj_name}'):
                os.chdir('pred_repos')
                subprocess.run(['git','clone',target_info[proj_name][0]['commits'][0]['repository']])
                os.chdir(BASEDIR)
            
            for commit in target_info[proj_name]:
                for commit_hash in commit['vuln_lines']:
                    os.chdir(f'./pred_repos/{proj_name}')
                    subprocess.run(['git','checkout', commit_hash,'-f'])
                    os.chdir(BASEDIR)
                    target_name = (proj_name + '_' + commit_hash).replace(':', '_')
                    if not os.path.isdir((f'./filtered_files/{target_name}')):
                        os.mkdir((f'./filtered_files/{target_name}'))
                    for path_info in commit['vuln_lines'][commit_hash]:
                        target = './pred_repos/' + proj_name + path_info.split(' ')[1][1:]
                        result_path = f'./filtered_files/{target_name}/'+'_'.join(target.split('/')[-3:])
                        if not os.path.isdir(result_path):
                            os.mkdir(result_path)
                        with open(target, 'r', errors='ignore') as fp:
                            with open(result_path+'/filtered.java', 'w') as fp2:
                                for line in fp.readlines():
                                    if not (line.startswith('import') or line.strip().startswith('*') or line.strip().startswith('/')):
                                        fp2.write(line)
    else:
        acceptance = [0.8]

        model = wv.Word2Vec.load("models/IDname.model")

        features = 100

        index2word_set = set(model.wv.index2word)

        readVulnLines(model, features, index2word_set)

        predict(args.target_file, model, features, index2word_set, acceptance)


if __name__ == '__main__':
    main()
